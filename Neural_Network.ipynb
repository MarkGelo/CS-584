{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "Neural Network.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVnMAPsQE9EI"
      },
      "source": [
        "# Assignment 3 - Multi-class Classification and Neural Network\n",
        "> **FULL MARKS = 10**\n",
        "\n",
        "In this assignment, you are going to implement your own neural network to do multi-class classification. We use one-vs-all strategy here by training multiple binary classifiers (one for each class).\n",
        "\n",
        "Please notice that you can only use numpy and scipy.optimize.minimize. **No** library versions of other method are allowed.  . Follow the instructions, you will need to fill the blanks to make it functional. The process is similar to the previous assignment. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9p0gBMdmE9Ef"
      },
      "source": [
        "from sklearn import datasets\n",
        "from scipy.optimize import minimize\n",
        "import numpy as np"
      ],
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O46iueD6E9Ek"
      },
      "source": [
        "def load_dataset():\n",
        "    iris = datasets.load_iris()\n",
        "    X = iris.data\n",
        "    y = iris.target\n",
        "    \n",
        "    return X, y"
      ],
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyNhiS08E9En"
      },
      "source": [
        "def train_test_split(X, y):\n",
        "    idx = np.arange(len(X))\n",
        "    train_size = int(len(X) * 2/3)\n",
        "    np.random.shuffle(idx)\n",
        "    X = X[idx]\n",
        "    y = y[idx]\n",
        "    X_train, X_test = X[:train_size], X[train_size:]\n",
        "    y_train, y_test = y[:train_size], y[train_size:]\n",
        "    \n",
        "    return X_train, X_test, y_train, y_test"
      ],
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbONElmJE9Et"
      },
      "source": [
        "def init_weights(num_in, num_out):\n",
        "    '''\n",
        "    :param num_in: the number of input units in the weight array\n",
        "    :param num_out: the number of output units in the weight array\n",
        "    '''\n",
        "\n",
        "    # Note that 'W' contains both weights and bias, you can consider W[0, :] as bias\n",
        "    W = np.zeros((1 + num_in, num_out))\n",
        "    \n",
        "    ###################################################################################\n",
        "    # Full Mark: 1                                                                    #\n",
        "    # TODO:                                                                           #\n",
        "    # Implement Xavier/Glorot uniform initialization                                  #\n",
        "    #                                                                                 #\n",
        "    # Hint: you can find the reference here:                                          #\n",
        "    # https://www.tensorflow.org/api_docs/python/tf/keras/initializers/GlorotUniform  #\n",
        "    ###################################################################################\n",
        "\n",
        "    lim = np.sqrt(6 / (num_in + num_out))\n",
        "    for i in range(0, num_in):\n",
        "      for j in range(num_out):\n",
        "        W[i, j] = np.random.uniform(-lim, lim)\n",
        "\n",
        "    ###################################################################################\n",
        "    #                       END OF YOUR CODE                                          #\n",
        "    ###################################################################################\n",
        "    \n",
        "    return W"
      ],
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pektf_JYE9Ev"
      },
      "source": [
        "def sigmoid(x):\n",
        "    '''\n",
        "    :param x: input\n",
        "    '''\n",
        "\n",
        "    ###################################################################################\n",
        "    # Full Mark: 0.5                                                                  #\n",
        "    # TODO:                                                                           #\n",
        "    # Implement sigmoid function:                                                     #\n",
        "    #                             sigmoid(x) = 1/(1+e^(-x))                           #\n",
        "    ###################################################################################\n",
        "\n",
        "    res = 1 / (1 + np.exp(-x))\n",
        "\n",
        "    ###################################################################################\n",
        "    #                       END OF YOUR CODE                                          #\n",
        "    ###################################################################################\n",
        "\n",
        "    return res"
      ],
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbYNxz0fE9Ez"
      },
      "source": [
        "def tanh(x):\n",
        "    '''\n",
        "    :param x: input\n",
        "    '''\n",
        "\n",
        "    ###################################################################################\n",
        "    # Full Mark: 0.5                                                                  #\n",
        "    # TODO:                                                                           #\n",
        "    # Implement tanh function:                                                        #\n",
        "    #                     tanh(x) = (e^x-e^(-x)) / (e^x+e^(-x))                       #\n",
        "    ###################################################################################\n",
        "\n",
        "    res = (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
        "\n",
        "    ###################################################################################\n",
        "    #                       END OF YOUR CODE                                          #\n",
        "    ###################################################################################\n",
        "\n",
        "    return res"
      ],
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPZ80cDCE9E0"
      },
      "source": [
        "def sigmoid_gradient(x):\n",
        "    '''\n",
        "    :param x: input\n",
        "    '''\n",
        "\n",
        "    ###################################################################################\n",
        "    # Full Mark: 1                                                                    #\n",
        "    # TODO:                                                                           #\n",
        "    # Computes the gradient of the sigmoid function evaluated at x.                   #\n",
        "    #                                                                                 #\n",
        "    ###################################################################################\n",
        "\n",
        "    p = sigmoid(x)\n",
        "    grad = p * (1 - p)\n",
        "\n",
        "    ###################################################################################\n",
        "    #                       END OF YOUR CODE                                          #\n",
        "    ###################################################################################\n",
        "\n",
        "    return grad"
      ],
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gmv-ePpSE9E1"
      },
      "source": [
        "def tanh_gradient(x):\n",
        "    '''\n",
        "    :param x: input\n",
        "    '''\n",
        "\n",
        "    ###################################################################################\n",
        "    # Full Mark: 1                                                                    #\n",
        "    # TODO:                                                                           #\n",
        "    # Computes the gradient of the tanh function evaluated at x.                      #\n",
        "    #                                                                                 #\n",
        "    ###################################################################################\n",
        "\n",
        "    grad = 1.0 - (tanh(x) ** 2)\n",
        "\n",
        "    ###################################################################################\n",
        "    #                       END OF YOUR CODE                                          #\n",
        "    ###################################################################################\n",
        "\n",
        "    return grad"
      ],
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXM45xLJE9E3"
      },
      "source": [
        "def forward(W, X):\n",
        "    '''\n",
        "    :param W: weights (including biases) of the neural network. It is a list containing both W_hidden and W_output.\n",
        "    :param X: input. Already added one additional column of all \"1\"s.\n",
        "    '''\n",
        "\n",
        "    # Shape of W_hidden: [num_feature+1, num_hidden]\n",
        "    # Shape pf W_output: [num_hidden+1, num_output]\n",
        "    W_hidden, W_output = W\n",
        "\n",
        "    ###################################################################################\n",
        "    # Full Mark: 1                                                                    #\n",
        "    # TODO:                                                                           #\n",
        "    # Implement the forward pass. You need to compute four values:                    #\n",
        "    # z_hidden, a_hidden, z_output, a_output                                          #\n",
        "    #                                                                                 #\n",
        "    # Note that our neural network consists of three layers:                          #\n",
        "    # Input -> Hidden -> Output                                                       #\n",
        "    # The activation function in hidden layer is 'tanh'                               #\n",
        "    # The activation function in output layer is 'sigmoid'                            #\n",
        "    ###################################################################################\n",
        "\n",
        "    #print(X[1])\n",
        "    #print(\"X Shape:\", X.shape)\n",
        "    #print(\"W Hidden Shape:\", W_hidden.shape)\n",
        "\n",
        "    z_hidden = np.dot(X, W_hidden)\n",
        "    a_hidden = tanh(z_hidden)\n",
        "\n",
        "    #print(\"Z Hidden Shape:\", z_hidden.shape)\n",
        "    #print(\"A Hidden Shape:\", a_hidden.shape)\n",
        "    #print(\"W Output Shape:\", W_output.shape)\n",
        "\n",
        "    a_hidden =  np.concatenate([np.ones((X.shape[0], 1)), a_hidden], axis=1) # add one row to a_hidden cuz Shape of W_output: [num_hidden+1, num_output]\n",
        "    z_output = np.dot(a_hidden, W_output)\n",
        "    a_output = sigmoid(z_output)\n",
        "\n",
        "    #print(\"Z Output Shape:\", z_output.shape)\n",
        "    #print(\"A Output Shape:\", a_output.shape)\n",
        "    \n",
        "    ###################################################################################\n",
        "    #                       END OF YOUR CODE                                          #\n",
        "    ###################################################################################\n",
        "\n",
        "    # z_hidden is the raw output of hidden layer, a_hidden is the result after performing activation on z_hidden\n",
        "    # z_output is the raw output of output layer, a_output is the result after performing activation on z_output\n",
        "    return {'z_hidden': z_hidden, 'a_hidden': a_hidden,\n",
        "            'z_output': z_output, 'a_output': a_output}"
      ],
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-pOCnn6E9E6"
      },
      "source": [
        "def loss_function(W, X, y, num_feature, num_hidden, num_output, L2_lambda):\n",
        "    '''\n",
        "    :param W: a 1D array containing all weights and biases.\n",
        "    :param X: input\n",
        "    :param y: labels of X\n",
        "    :param num_feature: number of features in X\n",
        "    :param num_hidden: number of hidden units\n",
        "    :param num_output: number of output units\n",
        "    :param L2_lambda: the coefficient of regularization term\n",
        "    '''\n",
        "\n",
        "    m = y.size\n",
        "\n",
        "    # Reshape W back into the parameters W_hidden and W_output\n",
        "    #print(W.shape)\n",
        "    W_hidden = np.reshape(W[:num_hidden * (num_feature + 1)],\n",
        "                          ((num_feature + 1), num_hidden))\n",
        "\n",
        "    W_output = np.reshape(W[(num_hidden * (num_feature + 1)):],\n",
        "                          ((num_hidden + 1), num_output))\n",
        "\n",
        "    # Initialize grads\n",
        "    W_hidden_grad = np.zeros(W_hidden.shape)\n",
        "    W_output_grad = np.zeros(W_output.shape)\n",
        "    #test_grads = np.concatenate([W_hidden_grad.ravel(), W_output_grad.ravel()])\n",
        "    #print(\"W HIDDEN GRAD:\", W_hidden_grad.shape)\n",
        "    #print(\"W OUTPUT GRAD:\", W_output_grad.shape)\n",
        "    #print(\"OUTPUT GRADS:\", test_grads.shape)\n",
        "    #print(W_output_grad.shape)\n",
        "\n",
        "    # Add one column of \"1\"s to X\n",
        "    X_input = np.concatenate([np.ones((m, 1)), X], axis=1)\n",
        "\n",
        "    ##########################################################################################\n",
        "    # Full Mark: 3                                                                           #\n",
        "    # TODO:                                                                                  #\n",
        "    # 1. Transform y to one-hot encoding. Implement binary cross-entropy loss function       #\n",
        "    # (Hint: Use the forward function to get necessary outputs from the model)               #\n",
        "    #                                                                                        #\n",
        "    # 2. Add L2 regularization to all weights in loss                                        #\n",
        "    # (Note that we will not add regularization to bias)                                     #\n",
        "    #                                                                                        #\n",
        "    # 3. Compute the gradient for W_hidden and W_output (including both weights and biases)  #\n",
        "    # (Hint: use chain rule, and the sigmoid gradient, tanh gradient you have                #\n",
        "    # implemented above. Don't forget to add the gradient of regularization term)            #\n",
        "    ##########################################################################################\n",
        "\n",
        "    # one hot encoding\n",
        "    classes = len(list(set(y))) # number of classes for one hot encoding\n",
        "    #y_hot = np.eye(classes)[y]\n",
        "    y_hot = np.zeros((len(y), classes))\n",
        "    for i in range(len(y)):\n",
        "      y_hot[i, y[i]] = 1\n",
        "\n",
        "\n",
        "    # cross entropy loss\n",
        "    fwd = forward((W_hidden, W_output), X_input)\n",
        "    A1 = fwd['a_hidden']\n",
        "    A2 = fwd['a_output']\n",
        "    z_output = fwd['z_output']\n",
        "    z_hidden = fwd['z_hidden']\n",
        "    #print(A2)\n",
        "    #print(fwd['a_output'][1])\n",
        "    L = (-1 / m) * np.sum(np.multiply(np.log(A2), y_hot) + np.multiply((1 - y_hot), np.log(1 - A2)))\n",
        "    #L = (1 / m) * np.sum((-np.dot(y_hot, np.log(A2).T) - np.dot(1 - y_hot, np.log(1 - A2).T)))\n",
        "    #print(\"Loss:\", L)\n",
        "\n",
        "\n",
        "    # add regularization to all weights in loss except bias -- bias is W[0, :]\n",
        "    L += (1 / (2 * m)) * np.sum(np.power(W_hidden[1:, :], 2)) + (1 / (2 * m)) * np.sum(np.power(W_output[1:, :], 2))\n",
        "\n",
        "\n",
        "    # compute gradient - W_hidden (5, 10) | W_output (11, 3)\n",
        "\n",
        "    # L = -y_hot * log(A2) - (1 - y_hot) * log(1 - A2)\n",
        "    # dL/A2 = -y_hot/A2 - (1 - y_hot)/(1 - A2)\n",
        "    dLA2 = -y_hot/A2 - (1 - y_hot)/(1 - A2)\n",
        "    # A2 = sigmoid(z_output)\n",
        "    # dA2_z_output = sigmoid_gradient(z_output)\n",
        "    # dL_z_output = dL_A2 * sigmoid_gradient(z_output)\n",
        "    dLZoutput = np.dot(dLA2.T, sigmoid_gradient(z_output))\n",
        "    # z_output = a_hidden * W_output\n",
        "    # dZ_output_A1 = w_output\n",
        "    # dL_A1 = w_output * dL_Z_output\n",
        "    dLA1 = (1 / m) * np.dot(W_output, dLZoutput.T)\n",
        "    #print(dLA1.shape) # (11, 3)\n",
        "    w_output_grad = dLA1\n",
        "    \n",
        "    # A1 = tanh(z_hidden)\n",
        "    # dA1_Z_hidden = tanh_gradient(z_hidden)\n",
        "    # dL_z_hidden = dL_A1 * tanh_gradient(z_hidden)\n",
        "    #print(\"dLA2\", dLA2.shape)\n",
        "    #print(\"dLZoutput\", dLZoutput.shape)\n",
        "    #print(\"dLA1\", dLA1.shape)\n",
        "    #print(\"X_input\", X_input.shape)\n",
        "    dLzHidden = np.dot(dLA1.T, tanh_gradient(A1).T)\n",
        "    #print(dLzHidden.shape)\n",
        "\n",
        "    # Z_hidden = w_hidden * X\n",
        "    # dz_hidden_w_hidden = X\n",
        "    # dL_w_hidden = X * dL_z_hidden\n",
        "\n",
        "    #dLwHidden = np.dot(X_input, dLzHidden)\n",
        "    #w_hidden_grad = dLwHidden\n",
        "\n",
        "    dZ2 = A2 - y_hot\n",
        "    dW2 = (1 / m) * np.dot(A1.T, dZ2)\n",
        "    #print(dW2.shape)\n",
        "    #W_hidden_grad = tanh_gradient(W_hidden)\n",
        "    W_output_grad = dW2\n",
        "    #W_hidden_grad = np.dot(np.dot((y - fwd['a_output']).T, sigmoid_gradient(W_output).T), tanh_gradient(W_hidden).T)\n",
        "    #print(\"A1\", A1.shape)\n",
        "    #print(\"dZ2\", dZ2.shape)\n",
        "    #print(\"W_hidden\", W_hidden.shape)\n",
        "    #print(\"W_output\", W_output.shape)\n",
        "    dZ1 = np.multiply(np.dot(W_output, dZ2.T).T, tanh_gradient(A1))\n",
        "    dW1 = (1 / m) * np.dot(X_input.T, dZ1[:, 1:])\n",
        "    #print(dW1.shape)\n",
        "    #W_output_grad = sigmoid_gradient(W_output)\n",
        "    W_hidden_grad = dW1\n",
        "    #W_output_grad = np.dot((y - fwd['a_output']).T, sigmoid_gradient(W_output).T)\n",
        "\n",
        "    #print(\"RESULTING W HIDDEN GRAD:\", W_hidden_grad.shape)\n",
        "    #print(\"RESULTING W OUTPUT GRAD:\", W_output_grad.shape)\n",
        "\n",
        "    \n",
        "    ###################################################################################\n",
        "    #                       END OF YOUR CODE                                          #\n",
        "    ###################################################################################\n",
        "\n",
        "    grads = np.concatenate([W_hidden_grad.ravel(), W_output_grad.ravel()])\n",
        "    #print(\"RESULTING GRADS:\", grads.shape)\n",
        "\n",
        "    return L, grads"
      ],
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEw4xxXpE9E_"
      },
      "source": [
        "def optimize(initial_W, X, y, num_epoch, num_feature, num_hidden, num_output, L2_lambda):\n",
        "    '''\n",
        "    :param initial_W: initial weights as a 1D array.\n",
        "    :param X: input\n",
        "    :param y: labels of X\n",
        "    :param num_epoch: number of iterations\n",
        "    :param num_feature: number of features in X\n",
        "    :param num_hidden: number of hidden units\n",
        "    :param num_output: number of output units\n",
        "    :param L2_lambda: the coefficient of regularization term\n",
        "    '''\n",
        "\n",
        "    options = {'maxiter': num_epoch}\n",
        "\n",
        "    ###########################################################################################\n",
        "    # Full Mark: 1                                                                            #\n",
        "    # TODO:                                                                                   #\n",
        "    # Optimize weights                                                                        #\n",
        "    # (Hint: use scipy.optimize.minimize to automatically do the iteration.)                  #\n",
        "    # ref: https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html) #\n",
        "    # For some optimizers, you need to set 'jac' as True.                                     #\n",
        "    # You may need to use lambda to create a function with one parameter to wrap the          #\n",
        "    # loss_funtion you have implemented above.                                                #\n",
        "    #                                                                                         #\n",
        "    # Note that scipy.optimize.minimize only accepts a 1D weight array as initial weights,    #\n",
        "    # and the output optimized weights will also be a 1D array.                               #\n",
        "    # That is why we unroll the initial weights into a single long vector.                    #\n",
        "    ###########################################################################################\n",
        "\n",
        "    # W, X, y, num_feature, num_hidden, num_output, L2_lambda\n",
        "    loss = lambda w: loss_function(w, X, y, num_feature, num_hidden, num_output, L2_lambda)\n",
        "    #W_final = minimize(fun = loss_function, x0 = initial_W, args = (X, y, num_feature, num_hidden, num_output, L2_lambda), \n",
        "    #                   jac = True, options = options)\n",
        "    optimized = minimize(fun = loss, x0 = initial_W, \n",
        "                       jac = True, options = options)\n",
        "    \n",
        "    #print(optimized)\n",
        "    \n",
        "    W_final = optimized.x\n",
        "\n",
        "    ###################################################################################\n",
        "    #                       END OF YOUR CODE                                          #\n",
        "    ###################################################################################\n",
        "\n",
        "    # Obtain W_hidden and W_output back from W_final\n",
        "    #print(W_final)\n",
        "    W_hidden = np.reshape(W_final[:num_hidden * (num_feature + 1)],\n",
        "                          ((num_feature + 1), num_hidden))\n",
        "    W_output = np.reshape(W_final[(num_hidden * (num_feature + 1)):],\n",
        "                          ((num_hidden + 1), num_output))\n",
        "\n",
        "    return [W_hidden, W_output]"
      ],
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnLx8asuE9FC"
      },
      "source": [
        "def predict(X_test, y_test, W):\n",
        "    '''\n",
        "    :param X_test: input\n",
        "    :param y_test: labels of X_test\n",
        "    :param W: a list containing two weights W_hidden and W_output.\n",
        "    '''\n",
        "\n",
        "    test_input = np.concatenate([np.ones((y_test.size, 1)), X_test], axis=1)\n",
        "\n",
        "    ###################################################################################\n",
        "    # Full Mark: 1                                                                    #\n",
        "    # TODO:                                                                           #\n",
        "    # Predict on test set and compute the accuracy.                                   #\n",
        "    # (Hint: use forward function to get predicted output)                            #\n",
        "    #                                                                                 #\n",
        "    ###################################################################################\n",
        "\n",
        "    fwd = forward(W, test_input)\n",
        "    pred_class = []\n",
        "    for res in fwd['a_output']:\n",
        "      pred = np.argmax(res, axis = 0)\n",
        "      pred_class.append(pred)\n",
        "    #print(y_test)\n",
        "    #print(fwd['a_output'])\n",
        "    #print(pred_class)\n",
        "    correct = 0\n",
        "    for i in range(len(pred_class)):\n",
        "      if pred_class[i] == y_test[i]:\n",
        "        correct += 1\n",
        "    acc = correct / len(y_test)\n",
        "\n",
        "    ###################################################################################\n",
        "    #                       END OF YOUR CODE                                          #\n",
        "    ###################################################################################\n",
        "\n",
        "    return acc"
      ],
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsVy5igxE9FD",
        "outputId": "cdebcab1-d3a6-441f-bf7d-eb742c4cd08c"
      },
      "source": [
        "# Do not modify this part #\n",
        "# Define parameters\n",
        "NUM_FEATURE = 4\n",
        "NUM_HIDDEN_UNIT = 10\n",
        "NUM_OUTPUT_UNIT = 3\n",
        "NUM_EPOCH = 100\n",
        "L2_lambda = 1\n",
        "\n",
        "# Load data\n",
        "X, y = load_dataset()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "# Initialize weights\n",
        "initial_W_hidden = init_weights(NUM_FEATURE, NUM_HIDDEN_UNIT)\n",
        "initial_W_output = init_weights(NUM_HIDDEN_UNIT, NUM_OUTPUT_UNIT)\n",
        "initial_W = np.concatenate([initial_W_hidden.ravel(), initial_W_output.ravel()], axis=0)\n",
        "\n",
        "# Neural network learning\n",
        "W = optimize(initial_W, X_train, y_train, NUM_EPOCH, NUM_FEATURE, NUM_HIDDEN_UNIT, NUM_OUTPUT_UNIT, L2_lambda)\n",
        "\n",
        "# Predict\n",
        "acc = predict(X_test, y_test, W)\n",
        "print(\"Test accuracy:\", acc)"
      ],
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.96\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}